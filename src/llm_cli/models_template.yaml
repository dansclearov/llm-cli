# LLM CLI User Model Configuration
# This file was auto-generated. Customize as needed.
#
# How this works:
# - This config is MERGED with the built-in models.yaml
# - You only need to add NEW models or OVERRIDE existing ones
# - Built-in aliases: sonnet, haiku, opus, gpt, gemini-pro, gemini-flash
# - Top-level keys starting with _ are ignored (use for YAML anchors or to disable providers)
#
# Override and custom alias examples:

# aliases:
#   default: haiku  # Override the default model
#   gpt: openai-responses/gpt-5  # Override built-in gpt alias
#   r1: openrouter/deepseek/deepseek-r1-0528  # Add custom aliases
#   r1-free: "openrouter/deepseek/deepseek-r1-0528:free"
#   llama: groq/llama-3.3-70b-versatile
#   kimi: moonshotai/kimi-latest

# -------------------------------------------------------------------
# Add new models from additional providers:

# OpenAI standard API (non-reasoning models):
# openai:
#   chatgpt-4o-latest: {}
#   gpt-4.5-preview: {}
#   gpt-4-turbo: {}

# More OpenAI Responses API models:
# openai-responses:
#   gpt-5:
#     supports_thinking: true
#     supports_search: true
#   gpt-5-mini:
#     supports_thinking: true
#     supports_search: true
#   o4-mini:
#     supports_thinking: true
#     supports_search: true
#   o3:
#     supports_thinking: true
#     supports_search: true

# OpenRouter models:
# _openrouter_min_fp8: &openrouter_min_fp8
#   provider:
#     quantizations: ["fp8", "fp16", "bf16", "fp32", "unknown"]
#
# openrouter:
#   "deepseek/deepseek-r1-0528:free":
#     supports_thinking: true
#     supports_search: true
#     extra_params:
#       <<: *openrouter_min_fp8
#   deepseek/deepseek-r1-0528:
#     supports_thinking: true
#     supports_search: true
#     extra_params:
#       <<: *openrouter_min_fp8
#   qwen/qwen3-max:
#     supports_search: true
#     extra_params:
#       <<: *openrouter_min_fp8

# xAI Grok models:
# grok:
#   grok-4:
#     supports_thinking: true
#   grok-4.1: {}

# Moonshot AI (Kimi) models:
# moonshotai:
#   kimi-latest:
#     supports_search: false
#   kimi-k2-thinking:
#     supports_thinking: true
#     supports_search: false

# Groq models:
# groq:
#   llama-3.3-70b-versatile: {}

# -------------------------------------------------------------------
# Additional pydantic-ai providers:
# azure, bedrock, cerebras, cohere, deepseek, fireworks, gateway,
# github, google-vertex, heroku, huggingface, litellm, mistral,
# nebius, ollama, outlines, ovhcloud, together, vercel
